version: "3"
x-airflow-common:
  &airflow-common
  
  #command: ["sh", "-c", "chown -R 50000:50000 /usr/local/airflow && chown airflow /usr/local/airflow/airflow/airflow.cfg"]

  image: docker-airflow2:latest
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 10
  volumes:
    - ./dags:/usr/local/airflow/dags
    - ./logs:/usr/local/airflow/logs
    - ./logs/scheduler:/usr/local/airflow/logs/scheduler
    - ./plugins:/usr/local/airflow/plugins
    - ./spark/app:/usr/local/spark/app # Spark Scripts (same path in airflow and spark)
    - ./spark/resources:/usr/local/spark/resources # Spark Resources (same path in airflow and spark)
  #user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
  depends_on:
    postgres:
      condition: service_healthy
services:
    namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
        container_name: namenode
        restart: always
        volumes:
            - hadoop_namenode:/hadoop/dfs/name
        environment:
            - CLUSTER_NAME=test
        ports:
            - 9870:9870
            - 9000:9000
        env_file:
            - ./hadoop.env
        networks:
            - hadoop

    datanode:
        image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
        container_name: datanode
        restart: always
        depends_on: 
            - namenode
        volumes:
            - hadoop_datanode:/hadoop/dfs/data
        environment:
            SERVICE_PRECONDITION: "namenode:9870"
        ports:
            - 9864:9864
        env_file:
            - ./hadoop.env
        networks:
            - hadoop

    resourcemanager:
        image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
        container_name: resourcemanager
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
        ports:
            - 8088:8088
        env_file:
            - ./hadoop.env
        networks:
            - hadoop
    
    nodemanager:
        image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
        container_name: nodemanager
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
        ports:
            - 8042:8042
        env_file:
            - ./hadoop.env
        networks:
            - hadoop
    
    historyserver:
        image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
        container_name: historyserver
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
        volumes:
            - hadoop_historyserver:/hadoop/yarn/timeline
        ports:
            - 8188:8188
        env_file:
            - ./hadoop.env
        networks:
            - hadoop
    
    hive-server:
        image: bde2020/hive:2.3.2-postgresql-metastore
        container_name: hive-server
        restart: always
        depends_on: 
            - namenode
            - datanode
        environment:
            HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
        ports:
            - 10000:10000
            - 10002:10002
        env_file:
            - ./hadoop.env
        networks:
            - hadoop
    
    hive-metastore:
        image: bde2020/hive:2.3.2-postgresql-metastore
        container_name: hive-metastore
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
        command: /opt/hive/bin/hive --service metastore
        ports:
            - 9083:9083
        env_file:
            - ./hadoop.env
        networks:
            - hadoop
    
    hive-metastore-postgresql:
        image: bde2020/hive-metastore-postgresql:2.3.0
        container_name: hive-metastore-postgresql
        volumes:
            - ./hive_metastore:/var/lib/postgresql/data
        networks:
            - hadoop
    

    spark-master:
        image: bde2020/spark-master:3.1.1-hadoop3.2
        container_name: spark-master
        ports:
            - "8079:8080"
            - "7077:7077"
        environment:
            - INIT_DAEMON_STEP=setup_spark
        volumes:
            - ../sandbox/spark/app:/usr/local/spark/app # Scripts (same path in airflow and spark)
            - ../sandbox/spark/resources:/usr/local/spark/resources # Resources (same path in airflow and spark)

    spark-worker-1:
        image: bde2020/spark-worker:3.1.1-hadoop3.2
        container_name: spark-worker-1
        depends_on:
            - spark-master
        ports:
            - "8081:8081"
        environment:
            - "SPARK_MASTER=spark://spark-master:7077"

    spark-worker-2:
        image: bde2020/spark-worker:3.1.1-hadoop3.2
        container_name: spark-worker-2
        depends_on:
            - spark-master
        ports:
            - "8084:8081"
        environment:
            - "SPARK_MASTER=spark://spark-master:7077"        

    spark-worker-3:
        image: bde2020/spark-worker:3.1.1-hadoop3.2
        container_name: spark-worker-3
        depends_on:
            - spark-master
        ports:
            - "8083:8081"
        environment:
            - "SPARK_MASTER=spark://spark-master:7077"        


    jupyter:
        image: jupyter/pyspark-notebook:spark-3.5.0 # pyspark
        container_name: jupyter
        ports:
        - 8888:8888
        volumes:
        - ./Notebook:/home/jovyan/work  # Mount a local directory to store notebooks
        - ./JDBC_Driver:/Drivers/SQL_Sever/jdbc

        environment:
        - SPARK_MASTER=spark://spark-master:7077  # Point to your Spark master
        networks:
        - hadoop
        

    sql-server:
      image: mcr.microsoft.com/mssql/server:2022-latest
      container_name: sql-server-container
      environment:
        MSSQL_SA_PASSWORD: Mo*012105
        ACCEPT_EULA: Y
      ports:
        - "1433:1433"
      volumes:
        - sql-server-data:/var/opt/mssql
      networks:
        - hadoop

 # Postgres used by Airflow
    postgres:
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        volumes:
            - ./postgres:/var/lib/postgresql/data
        ports:
            - 5432:5432
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        restart: always

    airflow-webserver:
        <<: *airflow-common
        command: webserver
        ports:
            - 8080:8080
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: always

    
    airflow-scheduler:
        <<: *airflow-common
        command: scheduler
        restart: always

    airflow-init:
        <<: *airflow-common
        command: version
        environment:
            <<: *airflow-common-env
            _AIRFLOW_DB_UPGRADE: 'true'
            _AIRFLOW_WWW_USER_CREATE: 'true'
            _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-elou}
            _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflowass}

    
    hue:
        image: gethue/hue:latest
        container_name: hue
        ports:
          - "8890:8888"
        depends_on:
          - hive-server
        networks:
          - hadoop

    powerbi: 
        image: quintoandar/powerbi-dashboards
        container_name: powerbi 
        ports: 
            - "8082:5000" 
        environment: 
            - ACCEPT_EULA=Y
    # Apache Superset 
    superset: 
        image: ukwa/superset:latest 
        networks: 
            - hadoop 
        ports: 
            - "8089:8088" # Port pour accéder à Superset depuis l'extérieur 
        environment: 
            - SUPERSET_ENV=production 
            - HIVE_THRIFT_URI=hive://hive-server:10000/default
            - SUPERSET_LOAD_EXAMPLES='false' # Option pour ne pas charger les exemples par défaut 
            - SUPERSET_SECRET_KEY= "DBnP7u1aCI3K1uvJInzfIMZyu6a1C8IsTyWLVub1Tg65F9x+8yPbvMlF"
#        command: "superset-init --username elou --firstname ElHadji --lastname Mboup --email mboupeh@ept.sn --password supersetass" # Commande pour créer l'administrateur

        depends_on: 
            - hive-metastore-postgresql # Dépendance à la base de données heive-metastore-PostgreSQL pour Superset 
        volumes: 
            - "../sandbox/superset:/home/superset" 


volumes:
    hadoop_namenode:
    hadoop_datanode:
    hadoop_historyserver:
    hive_metastore:
    sql-server-data:
    postgres:
    
networks:
    hadoop:
        name: hadoop
